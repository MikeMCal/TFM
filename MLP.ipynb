{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67331f2c-a6a5-4e77-857c-78a01a96d6d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab as pl\n",
    "\n",
    "from numpy import linalg\n",
    "#from qiskit_machine_learning.datasets import ad_hoc_data\n",
    "#from qiskit_machine_learning.kernels import FidelityQuantumKernel\n",
    "from qiskit.circuit.library import ZZFeatureMap\n",
    "from qiskit.primitives import Sampler\n",
    "from qiskit_algorithms.state_fidelities import ComputeUncompute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0eec36d-ae07-432d-b489-f7226319807f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Perceptron(object):\n",
    "\t# Este es el método del constructor. Define el número máximo de iteraciones T (épocas) que realizará el perceptrón durante el entrenamiento. Por defecto, se establece en 1.\n",
    "    def __init__(self, T=1):\n",
    "        self.T = T\n",
    "\t\n",
    "\t#Este método entrena el perceptrón ajustando los pesos w y el sesgo b a partir de los datos de entrenamiento X y las etiquetas de clase y.\n",
    "    def fit(self, X, y):\n",
    "\t#Obtiene el número de muestras (n_samples) y el número de características (n_features) de los datos de entrenamiento.\n",
    "        n_samples, n_features = X.shape\n",
    "\t#Inicializa el vector de pesos w como un vector de ceros con la misma dimensión que el número de características.\n",
    "        self.w = np.zeros(n_features, dtype=np.float64)\n",
    "\t#Inicializa el sesgo b como cero.\n",
    "        self.b = 0.0\n",
    "\t# Realiza un bucle sobre el número máximo de iteraciones (T).\n",
    "        for t in range(self.T):\n",
    "\t#Para cada muestra en los datos de entrenamiento:\n",
    "            for i in range(n_samples):\n",
    "\t#Verifica si la predicción del perceptrón para la muestra actual es diferente de la etiqueta de clase verdadera.\n",
    "                if self.predict(X[i])[0] != y[i]:\n",
    "\t#Actualiza los pesos w si la predicción es incorrecta multiplicando la etiqueta de clase por la muestra y sumándola a los pesos.\n",
    "                    self.w += y[i] * X[i]\n",
    "\t# Actualiza el sesgo b sumando la etiqueta de clase.\n",
    "                    self.b += y[i]\n",
    "\t\n",
    "\t#Este método calcula la proyección de las muestras X en el hiperplano de separación definido por los pesos w y el sesgo b. Devuelve el resultado de la multiplicación de X por w y sumando b.\n",
    "    def project(self, X):\n",
    "        return np.dot(X, self.w) + self.b\n",
    "\t\n",
    "\t#Este método predice las etiquetas de clase para las muestras X dadas.\n",
    "    def predict(self, X):\n",
    "\t#Convierte X en una matriz bidimensional si no lo es.\n",
    "        X = np.atleast_2d(X)\n",
    "\t#Devuelve el signo de la proyección de las muestras X, que es la predicción del perceptrón (-1 o 1).\n",
    "        return np.sign(self.project(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "364d6f48-434b-44e4-b8c2-8675619e3ab8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class MultilayerPerceptron(object):\n",
    "    #Toma tres argumentos obligatorios: layers, una lista que especifica el número de neuronas en cada capa; activation, que especifica la función de activación a usar (por defecto, 'relu'); y T, que especifica el número de iteraciones de entrenamiento (por defecto, 1).\n",
    "    def __init__(self, layers, activation='relu', T=1):\n",
    "        self.layers = layers         #Guarda la lista de capas\n",
    "        self.activation = activation #Guarda el tipo de función de activación\n",
    "        self.T = T                   #Guarda el número de iteraciones de entrenamiento\n",
    "        self.hidden_layers = []      #Inicializa una lista vacía para contener los pesos de las capas ocultas.\n",
    "\n",
    "        # Initialize hidden layers\n",
    "        for i in range(1, len(layers)):\n",
    "            self.hidden_layers.append(np.random.randn(layers[i-1] + 1, layers[i])) #Agrega a self.hidden_layers matrices de pesos aleatorias para cada capa oculta, incluyendo una fila extra para el sesgo.\n",
    "    \n",
    "    #toma un vector x como entrada y devuelve la salida después de aplicar la función de activación especificada.\n",
    "    def _activate(self, x):\n",
    "        if self.activation == 'relu':\n",
    "            return np.maximum(x, 0)\n",
    "        elif self.activation == 'sigmoid':\n",
    "            return 1 / (1 + np.exp(-x))\n",
    "        elif self.activation == 'tanh': #tangente hiperbólica\n",
    "            return np.tanh(x)\n",
    "    \n",
    "    #las funciones que siguen realizan la propagación hacia adelante, el entrenamiento mediante retropropagación y la predicción, respectivamente\n",
    "    def _forward_pass(self, X):\n",
    "        activations = [X]\n",
    "        input_data = X\n",
    "\n",
    "        # Forward pass through each hidden layer\n",
    "        for layer in self.hidden_layers:\n",
    "            # Add bias term to input data\n",
    "            input_data = np.hstack((input_data, np.ones((input_data.shape[0], 1))))\n",
    "            # Compute activations of the layer\n",
    "            output = self._activate(np.dot(input_data, layer))\n",
    "            print(\"layer : \", layer)\n",
    "            activations.append(output)\n",
    "            # Update input data for the next layer\n",
    "            input_data = output\n",
    "\n",
    "        return activations\n",
    "\n",
    "    def fit(self, X, y):\n",
    "    # Iterar sobre el número de épocas\n",
    "        for t in range(self.T):\n",
    "            # Iterar sobre cada muestra X en los datos de entrada\n",
    "            for i in range(len(X)):\n",
    "                # Realizar un pase hacia adelante para obtener las activaciones de cada capa para la muestra actual X[i:i+1].\n",
    "                activations = self._forward_pass(X[i:i+1])\n",
    "                output = activations[-1]  # Salida del modelo\n",
    "\n",
    "                # Calcular el error y la delta para la retropropagación\n",
    "                error = y[i] - output\n",
    "                delta = error\n",
    "                #print(delta)\n",
    "                #  #itera sobre cada capa oculta (layer) y sus activaciones correspondientes (activation). Se itera en reversa para comenzar desde la capa de salida y retrocede hacia las capas ocultas.\n",
    "                for layer, activation in zip(reversed(self.hidden_layers), reversed(activations[:-1])):\n",
    "                    # Se agrega un término de sesgo (unidades de sesgo) a las activaciones de la capa actual\n",
    "                    activation_with_bias = np.hstack((activation, np.ones((activation.shape[0], 1))))\n",
    "                    # #calcula el delta para la capa actual utilizando la matriz de pesos de la capa (layer) transpuesta y se multiplica para propagar el error hacia atrás. \n",
    "                    delta = np.dot(delta, layer.T) * (activation_with_bias > 0)\n",
    "                    # Actualizar los pesos de la capa utilizando el producto punto entre las activaciones de la capa anterior y el delta calculado. Esto ajusta los pesos para minimizar el error en la capa actual durante el proceso de retropropagación.\n",
    "                    layer += np.dot(activation, delta.T)\n",
    "\n",
    "                # Imprimir las formas de las matrices activations[-2].T y delta para depuración\n",
    "                print(\"Shape de activations[-2].T:\", activations[-2].T.shape)\n",
    "                print(\"Shape de delta:\", delta.shape)\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        activations = self._forward_pass(X)\n",
    "        return activations[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "5f037d7c-4552-4bfa-a7d2-aba80bdb9e66",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n   def fit(self, X, y):\\n       for t in range(self.T):\\n           #print(\"T: \", self.T) T=100\\n           for i in range(len(X)):                        #itera sobre cada muestra en el conjunto de datos de entrenamiento X.\\n               activations = self._forward_pass(X[i:i+1]) #realiza un pase hacia adelante (_forward_pass) en el modelo para obtener las activaciones de cada capa para la muestra actual X[i:i+1].\\n               output = activations[-1]                   #almacena las activaciones de la capa de salida, que representan las predicciones del modelo para la muestra actual.\\n               \\n               # Backpropagation\\n               error = y[i] - output                      #Calcula el error entre la etiqueta verdadera y[i] y la salida predicha output.\\n               delta = error                              #Inicializa el delta, que es la cantidad que se utilizará para actualizar los pesos durante el paso de retropropagación\\n               for layer, activation in zip(reversed(self.hidden_layers), reversed(activations[:-1])): #itera sobre cada capa oculta (layer) y sus activaciones correspondientes (activation). Se itera en reversa para comenzar desde la capa de salida y retrocede hacia las capas ocultas.\\n                   activation = np.hstack((activation, np.ones((activation.shape[0], 1)))) # Se agrega un término de sesgo (unidades de sesgo) a las activaciones de la capa actual, lo que nos permite modelar mejor las funciones no lineales\\n                   delta = np.dot(delta, layer.T) * (activation > 0) #calcula el delta para la capa actual utilizando la matriz de pesos de la capa (layer) transpuesta y se multiplica para propagar el error hacia atrás. \\n\\n                   print(\"Shape de activations[-2].T:\", activations[-2].T.shape)\\n                   print(\"Shape de delta:\", delta.shape)\\n                    #Actualiza los pesos de la capa actual utilizando el producto punto entre las activaciones de la capa anterior y el delta calculado. Esto ajusta los pesos para minimizar el error en la capa actual durante el proceso de retropropagación.\\n                       #activación capa anterior transpuesta\\n                   #n12 = np.squeeze(np.asarray(activations[-2].T))\\n                       #cantidad de error que se retropropaga\\n                   #n13 = np.squeeze(np.asarray(delta))\\n                   layer += np.dot(activations[-2].T, delta)\\n'"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " #función fit anterior\n",
    "\"\"\"\n",
    "    def fit(self, X, y):\n",
    "        for t in range(self.T):\n",
    "            #print(\"T: \", self.T) T=100\n",
    "            for i in range(len(X)):                        #itera sobre cada muestra en el conjunto de datos de entrenamiento X.\n",
    "                activations = self._forward_pass(X[i:i+1]) #realiza un pase hacia adelante (_forward_pass) en el modelo para obtener las activaciones de cada capa para la muestra actual X[i:i+1].\n",
    "                output = activations[-1]                   #almacena las activaciones de la capa de salida, que representan las predicciones del modelo para la muestra actual.\n",
    "                \n",
    "                # Backpropagation\n",
    "                error = y[i] - output                      #Calcula el error entre la etiqueta verdadera y[i] y la salida predicha output.\n",
    "                delta = error                              #Inicializa el delta, que es la cantidad que se utilizará para actualizar los pesos durante el paso de retropropagación\n",
    "                for layer, activation in zip(reversed(self.hidden_layers), reversed(activations[:-1])): #itera sobre cada capa oculta (layer) y sus activaciones correspondientes (activation). Se itera en reversa para comenzar desde la capa de salida y retrocede hacia las capas ocultas.\n",
    "                    activation = np.hstack((activation, np.ones((activation.shape[0], 1)))) # Se agrega un término de sesgo (unidades de sesgo) a las activaciones de la capa actual, lo que nos permite modelar mejor las funciones no lineales\n",
    "                    delta = np.dot(delta, layer.T) * (activation > 0) #calcula el delta para la capa actual utilizando la matriz de pesos de la capa (layer) transpuesta y se multiplica para propagar el error hacia atrás. \n",
    "\n",
    "                    print(\"Shape de activations[-2].T:\", activations[-2].T.shape)\n",
    "                    print(\"Shape de delta:\", delta.shape)\n",
    "                     #Actualiza los pesos de la capa actual utilizando el producto punto entre las activaciones de la capa anterior y el delta calculado. Esto ajusta los pesos para minimizar el error en la capa actual durante el proceso de retropropagación.\n",
    "                        #activación capa anterior transpuesta\n",
    "                    #n12 = np.squeeze(np.asarray(activations[-2].T))\n",
    "                        #cantidad de error que se retropropaga\n",
    "                    #n13 = np.squeeze(np.asarray(delta))\n",
    "                    layer += np.dot(activations[-2].T, delta)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "ef63b17a-4085-4d45-bd11-342e17c6bb4b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer :  [[-0.64099987 -1.06375071  0.33769469  0.77439983  0.87960593]\n",
      " [ 0.3133037   2.15749291  1.7577223   0.4117472  -0.66353118]\n",
      " [-0.61032138  1.36858309  1.4541339   0.76560783 -0.44034834]\n",
      " [-0.47538245 -1.14380683 -0.37958764  0.59168973  0.62755257]\n",
      " [ 0.16692007 -0.61051128 -0.25370815 -1.19808516  0.02056355]]\n",
      "layer :  [[ 1.79979069  1.35948263  0.5534546 ]\n",
      " [-0.65721374  0.10114862  2.48782436]\n",
      " [-0.06732012  0.72600741  0.56255048]\n",
      " [ 0.71045091 -0.45691951  1.14849707]\n",
      " [-1.62002644  0.92300954  1.1214564 ]\n",
      " [-0.41418795 -0.76203143  2.03538068]]\n",
      "Traceback \u001b[1;36m(most recent call last)\u001b[0m:\n",
      "\u001b[0m  Cell \u001b[0;32mIn[108], line 21\u001b[0m\n",
      "    mlp.fit(X_train, y_train)\u001b[0m\n",
      "\u001b[1;36m  Cell \u001b[1;32mIn[106], line 63\u001b[1;36m in \u001b[1;35mfit\u001b[1;36m\n",
      "\u001b[1;33m    layer += np.dot(activation, delta.T)\u001b[1;36m\n",
      "\u001b[1;31mValueError\u001b[0m\u001b[1;31m:\u001b[0m shapes (1,5) and (6,1) not aligned: 5 (dim 1) != 6 (dim 0)\n",
      "\n",
      "Use %tb to get the full traceback.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       ".button {\n",
       "  border: none;\n",
       "  color: white;\n",
       "  padding: 4px 8px;\n",
       "  text-align: center;\n",
       "  text-decoration: none;\n",
       "  display: inline-block;\n",
       "  font-size: 12px;\n",
       "  margin: 4px 2px;\n",
       "  transition-duration: 0.2s;\n",
       "  cursor: pointer;\n",
       "}\n",
       ".iqx-button {\n",
       "  background-color: #0f62fe; \n",
       "  color: white; \n",
       "}\n",
       ".iqx-button:hover {\n",
       "  background-color: #0043ce;\n",
       "  color: white;\n",
       "}\n",
       "</style>\n",
       "<a href=\"https://stackoverflow.com/search?q=ValueError: shapes (1,5) and (6,1) not aligned: 5 (dim 1) != 6 (dim 0)\" target='_blank'><button class='button iqx-button'>Search for solution online</button></a>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Paso 1: Cargar y preparar los datos\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Escalado de características\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# División en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Paso 2: Crear una instancia del MLP\n",
    "mlp = MultilayerPerceptron(layers=[4, 5, 3], activation='relu', T=100)\n",
    "\n",
    "# Paso 3: Entrenar el MLP\n",
    "mlp.fit(X_train, y_train)\n",
    "\n",
    "# Paso 4: Evaluar el rendimiento\n",
    "predictions = mlp.predict(X_test)\n",
    "\n",
    "# Imprime la precisión del MLP\n",
    "accuracy = (predictions == y_test).mean()\n",
    "print(\"Precisión del MLP:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bb9235-b521-4236-83de-4d043d09c851",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Qiskit v1.0.2 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
